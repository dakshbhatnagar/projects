{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping Data Using Selenium (w/o any exe file downloads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install selenium pandas beautifulsoup4 numpy --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing libraries that will help us scrape the data off the internet\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service as ChromeService\n",
    "from selenium.webdriver.common.by import By\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1st Task : Scraping Data from [screener.in](https://www.screener.in)\n",
    "\n",
    "This script automatically gathers important financial information for a group of companies from a website. Here's what it does in simple terms:\n",
    "\n",
    "1. **List of Companies**:\n",
    "   - We start with a list of company names, like **TCS**, **Reliance**, and **Titan**, for which we want to collect financial details.\n",
    "\n",
    "2. **Opening the Website Automatically**:\n",
    "   - The script uses an automation tool to automatically open a web browser (like Google Chrome) and go to the webpage of each company on [screener.in](https://www.screener.in)\n",
    "   - It sets up the browser to make sure everything runs smoothly, even if there are issues like slow loading or website restrictions.\n",
    "\n",
    "3. **Collecting Financial Information**:\n",
    "   - Once the companyâ€™s page is open, the script looks through the page to find important financial numbers, like profits, stock prices, or ratios that help investors understand how the company is doing.\n",
    "   - It picks out the key pieces of information from the page and saves them.\n",
    "\n",
    "4. **Organizing the Data**:\n",
    "   - For each company, it creates a table with the financial details (like a mini-report for each company).\n",
    "   - It then adds all these mini-reports together into one big report that includes all the companies in the list.\n",
    "\n",
    "5. **Finishing Up**:\n",
    "   - After collecting all the data, the browser is closed.\n",
    "   - The script also gives updates while running, letting you know when it has finished gathering data for each company.\n",
    "   - Once Completed, the script also saves a CSV file in the current working directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraped data for ULTRACEMCO...\n",
      "Scraped data for TITAN...\n",
      "Scraped data for TCS...\n",
      "Scraped data for SRF...\n",
      "Scraped data for SCHAEFFLER...\n",
      "Scraped data for RELIANCE...\n",
      "Scraped data for KOTAKBANK...\n",
      "Scraped data for HDFCBANK...\n",
      "Scraped data for EICHERMOT...\n",
      "Scraped data for DRREDDY...\n",
      "Scraped data for DIVISLAB...\n",
      "Scraped data for TATAELXSI...\n",
      "Scraped data for M&M...\n",
      "Scraped data for INFY...\n",
      "Scraped data for HINDUNILVR...\n",
      "Scraped data for CDSL...\n",
      "Data scraped and saved as a CSV file in the current working directory\n"
     ]
    }
   ],
   "source": [
    "# List of stock symbols to scrape\n",
    "symbols = ['ULTRACEMCO', 'TITAN', 'TCS', 'SRF', 'SCHAEFFLER', 'RELIANCE', 'KOTAKBANK', 'HDFCBANK',\n",
    "           'EICHERMOT', 'DRREDDY', 'DIVISLAB', 'TATAELXSI', 'M&M', 'INFY', 'HINDUNILVR', 'CDSL']\n",
    "\n",
    "# Create an empty DataFrame to store all the scraped data\n",
    "main = pd.DataFrame()\n",
    "\n",
    "# Loop through each symbol to scrape data\n",
    "for symbol in symbols:\n",
    "    # Set up Chrome options\n",
    "    chrome_options = Options()\n",
    "    chrome_options.add_argument(\"--headless\")\n",
    "    chrome_options.add_argument(\"--disable-gpu\")  # Disable GPU hardware acceleration\n",
    "    chrome_options.add_argument(\"--no-sandbox\")  # Disable sandboxing\n",
    "    chrome_options.add_argument(\"--disable-dev-shm-usage\")  # Disable /dev/shm usage\n",
    "    chrome_options.add_argument(\"--user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\")  # Set user-agent\n",
    "\n",
    "    # Initialize the Chrome WebDriver with the specified options\n",
    "    driver = webdriver.Chrome(options=chrome_options)\n",
    "\n",
    "    # Open the web page for the current symbol\n",
    "    driver.get(f\"https://www.screener.in/company/{symbol}/\")\n",
    "\n",
    "    # Wait for the page to fully load\n",
    "    time.sleep(5)\n",
    "\n",
    "    # Get the page source after the page has loaded\n",
    "    page_source = driver.page_source\n",
    "\n",
    "    # Parse the page source using BeautifulSoup\n",
    "    soup = BeautifulSoup(page_source, 'html.parser')\n",
    "\n",
    "    # Find all div elements with class 'company-ratios' (where the data is located)\n",
    "    search_results = soup.find_all('div', class_='company-ratios')\n",
    "\n",
    "    # Initialize lists to hold the metric names and values\n",
    "    values = []\n",
    "    numbers = []\n",
    "\n",
    "    # Iterate over each 'company-ratios' div found\n",
    "    for result in search_results:\n",
    "        # Find all list items within the 'company-ratios' div\n",
    "        items = result.find_all('li', class_='flex flex-space-between')\n",
    "        for item in items:\n",
    "            # Extract the metric name and value\n",
    "            value = item.find('span', class_='name').text.strip()\n",
    "            number = item.find('span', class_='number').text.strip()\n",
    "\n",
    "            # Add the extracted data to the lists\n",
    "            values.append(value)\n",
    "            numbers.append(number)\n",
    "\n",
    "    # Create a dictionary with the scraped data\n",
    "    data = {'Symbol': symbol, 'Metric': values, 'Numbers': numbers}\n",
    "\n",
    "    # Convert the dictionary to a DataFrame\n",
    "    df = pd.DataFrame(data)\n",
    "\n",
    "    # Append the current DataFrame to the main DataFrame\n",
    "    main = pd.concat([main, df], axis=0)\n",
    "\n",
    "    # Print a message indicating successful data scraping for the current symbol\n",
    "    print(f\"Scraped data for {symbol}...\")\n",
    "\n",
    "# Close the WebDriver after all symbols have been processed\n",
    "driver.quit()\n",
    "\n",
    "#Removing the Apostrophe sign\n",
    "main['Numbers'] = main['Numbers'].str.replace(\",\", \"\")\n",
    "\n",
    "#Saving Screener data as a CSV File\n",
    "main.to_csv('Screener_data.csv', index=False)\n",
    "\n",
    "print('Data scraped and saved as a CSV file in the current working directory')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Symbol</th>\n",
       "      <th>Metric</th>\n",
       "      <th>Numbers</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ULTRACEMCO</td>\n",
       "      <td>Market Cap</td>\n",
       "      <td>340077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ULTRACEMCO</td>\n",
       "      <td>Current Price</td>\n",
       "      <td>11780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ULTRACEMCO</td>\n",
       "      <td>High / Low</td>\n",
       "      <td>12078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ULTRACEMCO</td>\n",
       "      <td>Stock P/E</td>\n",
       "      <td>49.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ULTRACEMCO</td>\n",
       "      <td>Book Value</td>\n",
       "      <td>2047</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Symbol         Metric Numbers\n",
       "0  ULTRACEMCO     Market Cap  340077\n",
       "1  ULTRACEMCO  Current Price   11780\n",
       "2  ULTRACEMCO     High / Low   12078\n",
       "3  ULTRACEMCO      Stock P/E    49.2\n",
       "4  ULTRACEMCO     Book Value    2047"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#All of the Screener Data collected in 1 table\n",
    "main.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping Data from [nseindia.com](https://www.nseindia.com)\n",
    "\n",
    "This script automates the process of scraping stock data for various companies from the National Stock Exchange (NSE) website using Python and Selenium.\n",
    "\n",
    "1. **Setup**: \n",
    "   - It first initializes an empty structure (DataFrame) to store the collected data.\n",
    "   - A list of stock symbols is defined, representing the companies we want to gather data for (e.g., ULTRACEMCO, TITAN, TCS).\n",
    "\n",
    "2. **Automated Web Browsing**: \n",
    "   - For each stock symbol, the script uses a browser automation tool (Selenium) to open the NSE website and retrieve the page that contains data for that specific stock.\n",
    "   - Chrome browser settings are configured to avoid issues related to loading the webpage (e.g., disabling GPU, handling sandboxing issues).\n",
    "\n",
    "3. **Data Scraping**: \n",
    "   - The script extracts tables of stock data from the webpage by parsing the HTML structure using BeautifulSoup.\n",
    "   - It identifies key metrics (e.g., market cap, volatility) and their corresponding values from the tables on the webpage.\n",
    "\n",
    "4. **Data Storage**:\n",
    "   - The extracted metrics and values are stored in a DataFrame format, a table-like structure that organizes data for analysis.\n",
    "   - Data for each stock symbol is appended to the main DataFrame.\n",
    "\n",
    "5. **Completion**: \n",
    "   - After scraping data for each symbol, the browser is closed, and a message is printed to indicate successful data extraction.\n",
    "   - Once Completed, the script also saves all of the data as a CSV file in the current working directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraped data for ULTRACEMCO...\n",
      "Scraped data for TITAN...\n",
      "Scraped data for TCS...\n",
      "Scraped data for SRF...\n",
      "Scraped data for SCHAEFFLER...\n",
      "Scraped data for RELIANCEP1...\n",
      "Scraped data for KOTAKBANK...\n",
      "Scraped data for HDFCBANK...\n",
      "Scraped data for EICHERMOT...\n",
      "Scraped data for DRREDDY...\n",
      "Scraped data for DIVISLAB...\n",
      "Scraped data for TATAELXSI...\n",
      "Scraped data for M%26M...\n",
      "Scraped data for INFY...\n",
      "Scraped data for HINDUNILVR...\n",
      "Scraped data for CDSL...\n",
      "NSE Data scraped and saved as a CSV file in the current working directory\n"
     ]
    }
   ],
   "source": [
    "# Create empty DataFrame to store the final results\n",
    "nse = pd.DataFrame()\n",
    "\n",
    "symbols = ['ULTRACEMCO', 'TITAN', 'TCS', 'SRF', 'SCHAEFFLER', 'RELIANCEP1', 'KOTAKBANK', 'HDFCBANK', 'EICHERMOT',\n",
    "           'DRREDDY', 'DIVISLAB', 'TATAELXSI','M%26M', 'INFY', 'HINDUNILVR', 'CDSL']\n",
    "\n",
    "for symbol in symbols:\n",
    "    chrome_options = Options()\n",
    "    chrome_options.add_argument(\"--headless\")\n",
    "    chrome_options.add_argument(\"--disable-gpu\")\n",
    "    chrome_options.add_argument(\"--no-sandbox\")\n",
    "    chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
    "    chrome_options.add_argument(\"--user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\")\n",
    "\n",
    "    # Initialize the driver\n",
    "    driver = webdriver.Chrome(options=chrome_options)\n",
    "    driver.get(f\"https://www.nseindia.com/get-quotes/equity?symbol={symbol}\")\n",
    "    time.sleep(5)  # Wait for the page to fully load\n",
    "\n",
    "    # Get the page source\n",
    "    page_source = driver.page_source\n",
    "\n",
    "    # Parse the page source with BeautifulSoup\n",
    "    soup = BeautifulSoup(page_source, 'html.parser')\n",
    "\n",
    "    # Locate the table with the class 'card-table' and scrape relevant data\n",
    "    search_results = soup.find_all('table', class_='card-table')\n",
    "\n",
    "    #Loop through the table and append the values to a list\n",
    "    scraped = []\n",
    "    for results in search_results:\n",
    "        rows = results.find_all('tbody')\n",
    "        for row in rows:\n",
    "            values = row.find_all('td')\n",
    "            data = [value.text.strip().replace(\",\", \"\") for value in values]\n",
    "            scraped.append(data)\n",
    "\n",
    "    # Only proceed if we have scraped data\n",
    "    if scraped:\n",
    "        #Putting the scraped data into a table\n",
    "        df = pd.DataFrame(scraped)[1:][:2].T\n",
    "\n",
    "        # Flatten and extract metrics and values\n",
    "        metrics = np.ravel([df.iloc[i].values for i in range(len(df)) if i % 2 == 0])\n",
    "        values = np.ravel([df.iloc[i].values for i in range(len(df)) if i % 2 != 0])\n",
    "\n",
    "        # Make sure both arrays are of the same length\n",
    "        if len(metrics) != len(values):\n",
    "            # Pad the shorter list with None to ensure equal length\n",
    "            if len(metrics) > len(values):\n",
    "                values = np.append(values, [None] * (len(metrics) - len(values)))\n",
    "            else:\n",
    "                metrics = np.append(metrics, [None] * (len(values) - len(metrics)))\n",
    "\n",
    "        # Create a dictionary for the current symbol\n",
    "        json_data = {'Symbol': symbol, 'Metrics': metrics, 'Values': values}\n",
    "        df = pd.DataFrame(json_data)\n",
    "\n",
    "        # Append to the main DataFrame\n",
    "        nse = pd.concat([nse, df], axis=0)\n",
    "        \n",
    "    # Print a message indicating successful data scraping for the current symbol\n",
    "    print(f\"Scraped data for {symbol}...\")\n",
    "    \n",
    "    # Close the browser for the current symbol\n",
    "    driver.quit()\n",
    "\n",
    "#Dropping any rows that contain null values\n",
    "nse = nse.dropna()\n",
    "#Saving the data as a CSV file\n",
    "nse.to_csv('nse.csv', index=False)\n",
    "print('NSE Data scraped and saved as a CSV file in the current working directory')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Symbol</th>\n",
       "      <th>Metrics</th>\n",
       "      <th>Values</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ULTRACEMCO</td>\n",
       "      <td>52 Week High</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ULTRACEMCO</td>\n",
       "      <td>Security VaR</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ULTRACEMCO</td>\n",
       "      <td>52 Week Low</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ULTRACEMCO</td>\n",
       "      <td>Index VaR</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ULTRACEMCO</td>\n",
       "      <td>Upper Band</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Symbol       Metrics Values\n",
       "0  ULTRACEMCO  52 Week High      -\n",
       "1  ULTRACEMCO  Security VaR      -\n",
       "2  ULTRACEMCO   52 Week Low      -\n",
       "3  ULTRACEMCO     Index VaR      -\n",
       "4  ULTRACEMCO    Upper Band      -"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Displaying top 5 rows\n",
    "nse.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
