{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c01305e7",
   "metadata": {},
   "source": [
    "# Naukri Job Scraper\n",
    "\n",
    "This notebook scrapes job listings from Naukri.com for Data Analyst positions in Delhi/NCR. It then stores the scraped data in a Pandas DataFrame and exports it to a Google Sheet and a CSV file.\n",
    "\n",
    "## Introduction\n",
    "\n",
    "This project aims to automate the process of collecting job data from Naukri.com. By scraping the website, we can gather valuable information about Data Analyst job openings in Delhi/NCR, including:\n",
    "\n",
    "* Job Title\n",
    "* Company Name\n",
    "* Experience Required\n",
    "* Job Link\n",
    "* Location\n",
    "* Salary\n",
    "\n",
    "This data can be used for various purposes, such as:\n",
    "\n",
    "* **Tracking job openings:** Monitor the latest Data Analyst job postings in Delhi/NCR.\n",
    "* **Analyzing job market trends:** Gain insights into salary ranges, company hiring patterns, and other relevant data.\n",
    "* **Automating job search:** Use the scraped data to identify potential job opportunities. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fc1c4425",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "32c8591b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import time\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service as ChromeService\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "from datetime import datetime, time\n",
    "\n",
    "chrome_options = Options()\n",
    "#chrome_options.add_argument(\"--headless\")\n",
    "chrome_options.add_argument(\"--disable-gpu\")  # Disable GPU hardware acceleration\n",
    "chrome_options.add_argument(\"--no-sandbox\")  # Disable sandboxing\n",
    "chrome_options.add_argument(\"--disable-dev-shm-usage\")  # Disable /dev/shm usage\n",
    "chrome_options.add_argument(\"--user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\")\n",
    "# Initialize the Chrome WebDriver with the specified options\n",
    "driver = webdriver.Chrome(options=chrome_options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4b306e94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page 1 loaded. Starting Scraping...\n",
      "Page 2 loaded. Starting Scraping...\n",
      "Page 3 loaded. Starting Scraping...\n",
      "Page 4 loaded. Starting Scraping...\n",
      "Page 5 loaded. Starting Scraping...\n",
      "Page 6 loaded. Starting Scraping...\n",
      "Page 7 loaded. Starting Scraping...\n",
      "Page 8 loaded. Starting Scraping...\n",
      "Page 9 loaded. Starting Scraping...\n",
      "Page 10 loaded. Starting Scraping...\n",
      "Length of the dataframe : 170\n"
     ]
    }
   ],
   "source": [
    "titles, names, experiences, links, locations, salaries = [], [], [], [], [], []\n",
    "for j in range(1,11,1):\n",
    "    url = f'https://www.naukri.com/data-analyst-jobs-in-delhi-ncr-{j}?k=data+analyst&l=delhi+%2F+ncr&experience=1&nignbevent_src=jobsearchDeskGNB'\n",
    "    driver.get(url)\n",
    "    time.sleep(3)\n",
    "    print(f'Page {j} loaded. Scraping...')\n",
    "    for i in range(20):\n",
    "        \n",
    "        #Fetching the desired data and storing them for each page\n",
    "        title_path = f'//*[@id=\"listContainer\"]/div[2]/div/div[{i}]/div/div[1]/a'\n",
    "        co_path = f'//*[@id=\"listContainer\"]/div[2]/div/div[{i}]/div/div[2]/span/a[1]'\n",
    "        exp_path = f'//*[@id=\"listContainer\"]/div[2]/div/div[{i}]/div/div[3]/div/span[1]/span/span'\n",
    "        link_path = f'//*[@id=\"listContainer\"]/div[2]/div/div[{i}]/div/div[1]/a'\n",
    "        location_path = f'//*[@id=\"listContainer\"]/div[2]/div/div[{i}]/div/div[3]/div/span[3]/span/span'\n",
    "        salary_path = f'//*[@id=\"listContainer\"]/div[2]/div/div[{i}]/div/div[3]/div/span[2]/span/span'\n",
    "        title = driver.find_elements(By.XPATH, title_path)\n",
    "        co_name = driver.find_elements(By.XPATH, co_path)\n",
    "        experience = driver.find_elements(By.XPATH, exp_path)\n",
    "        link = driver.find_elements(By.XPATH, link_path)\n",
    "        location = driver.find_elements(By.XPATH, location_path)\n",
    "        salary = driver.find_elements(By.XPATH, salary_path)\n",
    "        \n",
    "        #Adding the text to the relevant lists\n",
    "        titles.extend([t.text for t in title])\n",
    "        names.extend([c.text for c in co_name])\n",
    "        experiences.extend([e.text for e in experience])\n",
    "        links.extend([lnk.get_attribute('href') for lnk in link])\n",
    "        locations.extend([loc.text for loc in location])\n",
    "        salaries.extend([sal.text for sal in salary])\n",
    "data = {'Titles': titles, 'Company': names, 'Experience': experiences,\n",
    "        'Links': links, 'Location': locations, 'Compensation': salaries}\n",
    "df = pd.DataFrame(data)\n",
    "df = df.drop_duplicates()\n",
    "df['Date_added'] = datetime.now().strftime('%Y-%m-%d')\n",
    "driver.quit()\n",
    "print(f'Length of the dataframe : {len(df)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "daa6e79f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Titles</th>\n",
       "      <th>Company</th>\n",
       "      <th>Experience</th>\n",
       "      <th>Links</th>\n",
       "      <th>Location</th>\n",
       "      <th>Compensation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Data Analyst (Tamil/ English)</td>\n",
       "      <td>Zbiz Solutions</td>\n",
       "      <td>0-3 Yrs</td>\n",
       "      <td>https://www.naukri.com/job-listings-data-analy...</td>\n",
       "      <td>Kolkata, Mumbai, New Delhi, Hyderabad, Pune, C...</td>\n",
       "      <td>Not disclosed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Data Analyst</td>\n",
       "      <td>Larko Tech</td>\n",
       "      <td>1-3 Yrs</td>\n",
       "      <td>https://www.naukri.com/job-listings-data-analy...</td>\n",
       "      <td>Kolkata, Mumbai, New Delhi, Hyderabad, Pune, C...</td>\n",
       "      <td>Not disclosed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Data Analyst(English Required)</td>\n",
       "      <td>Peroptyx</td>\n",
       "      <td>0-5 Yrs</td>\n",
       "      <td>https://www.naukri.com/job-listings-data-analy...</td>\n",
       "      <td>Remote</td>\n",
       "      <td>Not disclosed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Data Analyst (Position Closed)</td>\n",
       "      <td>Bluslash Consulting</td>\n",
       "      <td>0-2 Yrs</td>\n",
       "      <td>https://www.naukri.com/job-listings-data-analy...</td>\n",
       "      <td>Gurugram</td>\n",
       "      <td>Not disclosed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Data Analyst</td>\n",
       "      <td>Centre for Knowledge &amp; Development (CKD)</td>\n",
       "      <td>1-4 Yrs</td>\n",
       "      <td>https://www.naukri.com/job-listings-data-analy...</td>\n",
       "      <td>New Delhi</td>\n",
       "      <td>Not disclosed</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           Titles                                   Company  \\\n",
       "0   Data Analyst (Tamil/ English)                            Zbiz Solutions   \n",
       "1                    Data Analyst                                Larko Tech   \n",
       "2  Data Analyst(English Required)                                  Peroptyx   \n",
       "3  Data Analyst (Position Closed)                       Bluslash Consulting   \n",
       "4                    Data Analyst  Centre for Knowledge & Development (CKD)   \n",
       "\n",
       "  Experience                                              Links  \\\n",
       "0    0-3 Yrs  https://www.naukri.com/job-listings-data-analy...   \n",
       "1    1-3 Yrs  https://www.naukri.com/job-listings-data-analy...   \n",
       "2    0-5 Yrs  https://www.naukri.com/job-listings-data-analy...   \n",
       "3    0-2 Yrs  https://www.naukri.com/job-listings-data-analy...   \n",
       "4    1-4 Yrs  https://www.naukri.com/job-listings-data-analy...   \n",
       "\n",
       "                                            Location   Compensation  \n",
       "0  Kolkata, Mumbai, New Delhi, Hyderabad, Pune, C...  Not disclosed  \n",
       "1  Kolkata, Mumbai, New Delhi, Hyderabad, Pune, C...  Not disclosed  \n",
       "2                                             Remote  Not disclosed  \n",
       "3                                           Gurugram  Not disclosed  \n",
       "4                                          New Delhi  Not disclosed  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9acc543b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#defining the function to write data to google sheets\n",
    "def write_data(spreadsheet_id, sheet_name, df):\n",
    "    from googleapiclient import discovery\n",
    "    from oauth2client.service_account import ServiceAccountCredentials\n",
    "    keys_path = 'keys.json'\n",
    "    scope = ['https://spreadsheets.google.com/feeds','https://www.googleapis.com/auth/drive']\n",
    "    creds = ServiceAccountCredentials.from_json_keyfile_name(keys_path, scope)\n",
    "    service = discovery.build('sheets', 'v4', credentials=creds)\n",
    "\n",
    "    cols = list(df.columns[df.columns.str.contains('Date|Time',regex=True)])\n",
    "    if cols:\n",
    "        for col in cols:\n",
    "            df[col] = df[col].astype(str)\n",
    "\n",
    "    input_values = [list(df.columns)] + df.values.tolist()\n",
    "    input_request = [{\n",
    "                'range': f\"{sheet_name}!R1C1:R{df.shape[0]+1}C{df.shape[1]}\",\n",
    "                'majorDimension' : \"ROWS\",\n",
    "                'values' : input_values\n",
    "            }]\n",
    "    body = {'valueInputOption' : \"RAW\", 'data' : input_request}\n",
    "    \n",
    "    result = service.spreadsheets().values().batchUpdate(spreadsheetId = spreadsheet_id,\n",
    "                                                            body=body).execute()\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e923cb7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sheet_id = 'sheet_id'\n",
    "response = write_data(sheet_id, 'Sheet1', df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "06297aff",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('data.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b1ffe18",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This notebook demonstrates a simple yet effective approach to scraping job data from Naukri.com. By automating this process, we can save time and effort while gaining valuable insights into the job market. \n",
    "\n",
    "**Further Improvements:**\n",
    "\n",
    "* **Error Handling:** Implement robust error handling to gracefully handle unexpected website changes or network issues.\n",
    "* **Pagination:** Handle pagination effectively to scrape data from multiple pages of job listings.\n",
    "* **Data Cleaning:** Implement more sophisticated data cleaning techniques to ensure data accuracy and consistency.\n",
    "* **Visualization:** Create visualizations to gain deeper insights from the scraped data.\n",
    "\n",
    "This project serves as a starting point for exploring web scraping techniques and their applications in job market."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
